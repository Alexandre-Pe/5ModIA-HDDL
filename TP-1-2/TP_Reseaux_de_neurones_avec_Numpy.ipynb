{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b3pjAUEk2LQ"
   },
   "source": [
    "# Construire et entraîner un perceptron multi-couches - étape par étape\n",
    "\n",
    "Dans ce TP, vous allez mettre en œuvre l'entraînement d'un réseau de neurones (perceptron multi-couches) à l'aide de la librairie **numpy**. Pour cela nous allons procéder par étapes successives. Dans un premier temps nous allons traiter le cas d'un perceptron mono-couche, en commençant par la passe *forward* de prédiction d'une sortie à partir d'une entrée et des paramètres du perceptron, puis en implémentant la passe *backward* de calcul des gradients de la fonction objectif par rapport aux paramètrès. A partir de là, nous pourrons tester l'entraînement à l'aide de la descente de gradient stochastique.\n",
    "\n",
    "Une fois ces étapes achevées, nous pourrons nous atteler à la construction d'un perceptron multi-couches, qui consistera pour l'essentiel en la composition de perceptrons mono-couche. \n",
    "\n",
    "Dans ce qui suit, nous adoptons les conventions de notation suivantes : \n",
    "\n",
    "- $(x, y)$ désignent un couple donnée/label de la base d'apprentissage ; $\\hat{y}$ désigne quant à lui la prédiction du modèle sur la donnée $x$.\n",
    "\n",
    "- L'indice $i$ indique la $i^{\\text{ème}}$ dimension d'un vecteur.\n",
    "\n",
    "- L'exposant $[l]$ désigne un objet associé à la $l^{\\text{ème}}$ couche.\n",
    "\n",
    "- L'exposant $(k)$ désigne un objet associé au $k^{\\text{ème}}$ exemple. \n",
    "   \n",
    "Exemple:  \n",
    "- $a^{(2)[3]}_5$ indique la 5ème dimension du vecteur d'activation du 2ème exemple d'entraînement (2), de la 3ème couche [3].\n",
    "\n",
    "\n",
    "Commençons par importer tous les modules nécessaires : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "R6LBs_NLla1a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JZIXefJlXSV"
   },
   "source": [
    "\n",
    "--- \n",
    "\n",
    "## Perceptron mono-couche\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azdcz3QV_k-r"
   },
   "source": [
    "### Perceptron mono-couche - passe *forward*\n",
    "\n",
    "Un perceptron mono-couche est un modèle liant une couche d'entrée (en vert, qui n'effectue pas d'opération) à une couche de sortie. Les neurones des deux couches sont connectés par des liaisons pondérées (les poids synaptiques) $W_{xy}$, et les neurones de la couche de sortie portent chacun un biais additif $b_y$. Enfin, une fonction d'activation $f$ est appliquée à l'issue de ces opérations pour obtenir la prédiction du réseau $\\hat{y}$. \n",
    "\n",
    "On a donc :\n",
    "\n",
    "$$\\hat{y} = f ( W_{xy} x + b_y )$$ \n",
    "\n",
    "On posera pour la suite :\n",
    "$$ z = W_{xy} x + b_y $$\n",
    "\n",
    "La figure montre une représentation de ces opérations sous forme de réseau de neurones (à gauche), mais aussi sous une forme fonctionnelle (à droite) qui permet de bien visualiser l'ordre des opérations.\n",
    "\n",
    "<!-- <img src=\"https://drive.google.com/uc?id=1RZeiaKue0GLXJr3HRtKkuP6GD8r6I1_Q\" height=300> -->\n",
    "<!-- <img src=\"https://drive.google.com/uc?id=1dnQ6SSdpEX1GDTgoNTrUwA3xjiP9rTYU\" height=250> -->\n",
    "<img src=\"https://docs.google.com/uc?export=download&id=1U4V-MwOatw4axK2u8sJxaasUMl6A3TPo\" height=300>\n",
    "<img src=\"https://docs.google.com/uc?export=download&id=14tq-pbbFLvBZU-8LGvgYA71vrWNmSK73\" height=250> \n",
    "\n",
    "Notez que les paramètres du perceptron, que nous allons ajuster par un processus d'optimisation, sont donc les poids synaptiques $W_{xy}$ et les biais $b_y$. Par commodité dans le code, nous considérerons également comme un paramètre le choix de la fonction d'activation.\n",
    "\n",
    "**Remarque importante** : on a ici simplifié les dimensions des tenseurs en considérant que les données étaient prédites une à une par le perceptron. En pratique, on traite souvent les données par *batch*, c'est-à-dire que les prédictions sont faites pour plusieurs données simultanément. Ici pour une taille de *batch* de $m$, cela signifie en fait que :\n",
    " \n",
    "$$ x \\in \\mathbb{R}^{4 \\times m} \\text{  et  } y \\in \\mathbb{R}^{5 \\times m}$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBtX2euQDSCS"
   },
   "source": [
    "Complétez la fonction `dense_layer_forward` qui calcule la prédiction  d'un perceptron mono-couche pour une entrée $x$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YGYbWrRfmIwx"
   },
   "outputs": [],
   "source": [
    "def dense_layer_forward(x, Wxy, by, activation):\n",
    "    \"\"\"\n",
    "    Réalise une unique étape forward de la couche dense telle que décrite dans la figure (2)\n",
    "\n",
    "    Arguments:\n",
    "    x -- l'entrée, tableau numpy de dimension (n_x, m).\n",
    "    Wxy -- Matrice de poids multipliant l'entrée, tableau numpy de shape (n_y, n_x)\n",
    "    by -- Biais additif ajouté à la sortie, tableau numpy de dimension (n_y, 1)\n",
    "    activation -- Chaîne de caractère désignant la fonction d'activation choisie : 'linear', 'sigmoid' ou 'relu'\n",
    "\n",
    "    Retourne :\n",
    "    y_pred -- prédiction, tableau numpy de dimension (n_y, m)\n",
    "    cache -- tuple des valeurs utiles pour la passe backward (rétropropagation du gradient), contient (x, z)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### A COMPLETER ### \n",
    "    # calcul de z\n",
    "    z = ...\n",
    "    # calcul de la sortie en appliquant la fonction d'activation\n",
    "    if activation == 'relu':\n",
    "      y = ...\n",
    "    elif activation == 'sigmoid':\n",
    "      y = ...\n",
    "    elif activation == 'linear':\n",
    "      y = ...\n",
    "    else:\n",
    "      print(\"Erreur : la fonction d'activation n'est pas implémentée.\")\n",
    "    \n",
    "    # sauvegarde du cache pour la passe backward\n",
    "    cache = (x, z)\n",
    "    \n",
    "    return y, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dCFTHOqD_Tp"
   },
   "source": [
    "Exécutez les lignes suivantes pour vérifier la validité de votre code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B6wlVU37on1k",
    "outputId": "b14b0334-704b-473e-933a-8f56b11d2a27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred.shape = \n",
      " (2, 10)\n",
      "----------------------------\n",
      "y_pred[1] =\n",
      " [0.         2.11983968 0.88583246 1.39272594 0.         2.92664609\n",
      " 0.         1.47890228 0.         0.04725575]\n",
      "----------------------------\n",
      "y_pred[1] =\n",
      " [0.10851642 0.89281659 0.70802939 0.80102707 0.21934644 0.94914804\n",
      " 0.24545321 0.81440672 0.48495927 0.51181174]\n",
      "----------------------------\n",
      "y_pred[1] =\n",
      " [-2.10598556  2.11983968  0.88583246  1.39272594 -1.26947904  2.92664609\n",
      " -1.12301093  1.47890228 -0.06018107  0.04725575]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "x_tmp = np.random.randn(3,10)\n",
    "Wxy = np.random.randn(2,3)\n",
    "by = np.random.randn(2,1)\n",
    "\n",
    "activation = 'relu'\n",
    "y_pred_tmp, cache_tmp = dense_layer_forward(x_tmp, Wxy, by, activation)\n",
    "\n",
    "print(\"y_pred.shape = \\n\", y_pred_tmp.shape)\n",
    "\n",
    "print('----------------------------')\n",
    "\n",
    "print(\"y_pred[1] =\\n\", y_pred_tmp[1])\n",
    "\n",
    "print('----------------------------')\n",
    "\n",
    "activation = 'sigmoid'\n",
    "y_pred_tmp, cache_tmp = dense_layer_forward(x_tmp, Wxy, by, activation)\n",
    "print(\"y_pred[1] =\\n\", y_pred_tmp[1])\n",
    "\n",
    "print('----------------------------')\n",
    "\n",
    "activation = 'linear'\n",
    "y_pred_tmp, cache_tmp = dense_layer_forward(x_tmp, Wxy, by, activation)\n",
    "print(\"y_pred[1] =\\n\", y_pred_tmp[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYbiDw8TptiN"
   },
   "source": [
    "**Affichage attendu**: \n",
    "```Python\n",
    "y_pred.shape = \n",
    " (2, 10)\n",
    "----------------------------\n",
    "y_pred[1] =\n",
    " [0.         2.11983968 0.88583246 1.39272594 0.         2.92664609\n",
    " 0.         1.47890228 0.         0.04725575]\n",
    "----------------------------\n",
    "y_pred[1] =\n",
    " [0.10851642 0.89281659 0.70802939 0.80102707 0.21934644 0.94914804\n",
    " 0.24545321 0.81440672 0.48495927 0.51181174]\n",
    "----------------------------\n",
    "y_pred[1] =\n",
    " [-2.10598556  2.11983968  0.88583246  1.39272594 -1.26947904  2.92664609\n",
    " -1.12301093  1.47890228 -0.06018107  0.04725575]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GypgZ8jBqooR"
   },
   "source": [
    "### Perceptron mono-couche - passe *backward*\n",
    "\n",
    "Dans les librairies d'apprentissage profond actuelles, il suffit d'implémenter la passe *forward*, et la passe *backward* est réalisée automatiquement, avec le calcul des gradients (différentiation automatique) et la mise à jour des paramètres. Il est cependant intéressant de comprendre comment fonctionne la passe *backward*, en l'implémentant sur un exemple simple.\n",
    "\n",
    "<!-- <img src=\"https://drive.google.com/uc?id=1MC8Nxu6BQnpB7cGLwunIbgx9s1FaGw81\" height=350> -->\n",
    "<img src=\"https://docs.google.com/uc?export=download&id=1bIk-7GppJzkP2HNJ9RMvhjPoBNuNX-yU\" height=350> \n",
    "\n",
    "Il faut calculer les dérivées par rapport à la fonction de perte pour ensuite mettre à jour les paramètres du réseau. Les équations de rétropropagation sont données ci-dessous (c'est un bon exercice que de les calculer à la main). \n",
    "\n",
    "\\begin{align}\n",
    "\\displaystyle  {dW_{xy}} &~=~ \\frac{\\partial J}{\\partial W_{xy}} ~=~ dz . x^{T}\\tag{1} \\\\[8pt]\n",
    "\\displaystyle db_{y} &~=~ \\frac{\\partial J}{\\partial b_y} ~=~ \\sum_{batch}dz\\tag{2} \\\\[8pt]\n",
    "\\displaystyle dx &~=~ \\frac{\\partial J}{\\partial x} ~=~ { W_{xy}}^T . dz \\tag{3}  \\\\[8pt]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Ici, $*$ indique une multiplication élément par élément tandis que l'absence de symbole indique une multiplication matricielle. Par ailleurs $dz$ désigne $\\frac{\\partial J}{\\partial z}=\\frac{\\partial J}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial z}$, $dW_{xy}$ désigne $\\frac{\\partial J}{\\partial W_{xy}}$, $db_y$ désigne $\\frac{\\partial J}{\\partial b_y}$ et $dx$ désigne $\\frac{\\partial J}{\\partial x}$ (ces noms ont été choisis pour être utilisables dans le code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wEi_y3W_rCMc"
   },
   "outputs": [],
   "source": [
    "def dense_layer_backward(dy_hat, Wxy, by, activation, cache):\n",
    "    \"\"\"\n",
    "    Implémente la passe backward de la couche dense.\n",
    "\n",
    "    Arguments :\n",
    "    dy_hat -- Gradient de la perte J par rapport à la sortie ŷ\n",
    "    cache -- dictionnaire python contenant des variables utiles (issu de dense_layer_forward())\n",
    "\n",
    "    Retourne :\n",
    "    gradients -- dictionnaire python contenant les gradients suivants :\n",
    "                        dx -- Gradients par rapport aux entrées, de dimension (n_x, m)\n",
    "                        dby -- Gradients par rapport aux biais, de dimension (n_y, 1)\n",
    "                        dWxy -- Gradients par rapport aux poids synaptiques Wxy, de dimension (n_y, n_x)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Récupérer le cache\n",
    "    (x, z) = cache\n",
    "\n",
    "    ### A COMPLETER ###   \n",
    "    # calcul de la sortie en appliquant l'activation\n",
    "    # dy_dz -- Gradient de la sortie ŷ par rapport à l'état caché z\n",
    "    if activation == 'relu':\n",
    "      dy_dz = ...\n",
    "    elif activation == 'sigmoid':\n",
    "      dy_dz = ...\n",
    "    else: # Activation linéaire\n",
    "      dy_dz = ... \n",
    "\n",
    "\n",
    "    # calculer le gradient de la perte par rapport à x\n",
    "    dx = ... \n",
    "\n",
    "    # calculer le gradient de la perte par rapport à Wxy\n",
    "    dWxy = ... \n",
    "\n",
    "    # calculer le gradient de la perte par rapport à by \n",
    "    dby = ... \n",
    "\n",
    "    ### FIN ###\n",
    "    \n",
    "    # Stocker les gradients dans un dictionnaire\n",
    "    gradients = {\"dx\": dx, \"dby\": dby, \"dWxy\": dWxy}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5KeDgyO-ZPJ"
   },
   "source": [
    "On peut maintenant créer une classe `DenseLayer`, qui comprend en attribut toutes les informations nécessaires à la description d'une couche dense, c'est-à-dire : \n",
    "\n",
    "*   Le nombre de neurones en entrée de la couche dense (input_size)\n",
    "*   Le nombre de neurones en sortie de la couche dense (output_size)\n",
    "*   La fonction d'activation choisie sur cette couche (activation)\n",
    "*   Les poids synaptiques de la couche dense, stockés dans une matrice de taille (output_size, input_size) (Wxy)\n",
    "*   Les biais de la couche dense, stockés dans un vecteur de taille (output_size, 1) (by)\n",
    "\n",
    "On ajoute également un attribut cache qui permettra de stocker les entrées de la couche dense (x) ainsi que les calculs intermédiaires (z) réalisés lors de la passe *forward*, afin d'être réutilisés pour la basse *backward*.\n",
    "\n",
    "A vous de compléter les 4 jalons suivants : \n",
    "\n",
    "*   **L'initialisation des paramètres** Wxy et by : Wxy doit être positionnée suivant [l'initialisation de Glorot](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform), et by est initialisée par un vecteur de zéros.\n",
    "*   **La fonction `forward`**, qui consiste simplement en un appel de la fonction `dense_layer_forward` implémentée précédemment.\n",
    "*   **La fonction `backward`**, qui consiste simplement en un appel de la fonction `dense_layer_backward` implémentée précédemment.\n",
    "*   Et enfin **la fonction `update_parameters`** qui applique la mise à jour de la descente de gradient en fonction d'un taux d'apprentissage (learning_rate) et des gradients calculés dans la passe *forward*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "u2K9dp1IL3yM"
   },
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "      self.input_size = input_size\n",
    "      self.output_size = output_size\n",
    "      self.activation = activation\n",
    "      self.cache = None  # Le cache sera mis à jour lors de la passe forward\n",
    "      limit = math.sqrt(6 / (input_size + output_size))\n",
    "    \n",
    "      ### A COMPLETER ###\n",
    "      self.Wxy = ... # https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform\n",
    "      self.by = ...\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "      y, cache = ...\n",
    "      self.cache = cache\n",
    "      return y\n",
    "\n",
    "    def backward(self, dy):\n",
    "      return ...\n",
    "\n",
    "    def update_parameters(self, gradients, learning_rate):\n",
    "      self.Wxy -= learning_rate * gradients[\"dWxy\"]\n",
    "      self.by  -= learning_rate * gradients[\"dby\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GlEB8K3Lani"
   },
   "source": [
    "### Entraînement par descente de gradient stochastique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KMcQzlskdI1"
   },
   "source": [
    "Pour entraîner notre modèle, nous devons mettre en place un optimiseur. Nous implémenterons la descente de gradient stochastique avec mini-batch. Il nous faut cependant au préalable implanter la fonction de coût que nous utiliserons pour évaluer la qualité de nos prédictions. \n",
    "\n",
    "Pour le moment, nous allons nous contenter d'une erreur quadratique moyenne, qui associée à une fonction d'activation linéaire (l'identité) permet de résoudre les problèmes de régression. \n",
    "\n",
    "La fonction de coût prend en entrée deux paramètres : la vérité-terrain *y_true* et la prédiction du modèle *y_pred*. Ces deux matrices sont de dimension $bs \\times output\\text{_}size$. La fonction retourne deux grandeurs : *loss* qui correspond à l'erreur quadratique moyenne des prédictions par rapport aux vérités-terrains, et *grad* au gradient de l'erreur quadratique moyenne par rapport aux prédictions. Autrement dit : \n",
    "$$ \\text{grad}  = \\frac{\\partial J_{mb}}{\\partial \\hat{y}}$$\n",
    "\n",
    "où $\\hat{y}$ correspond à *y_pred*, et $J_{mb}$ à la fonction objectif calculée sur un mini-batch $mb$ de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FRDUnhJma6jf"
   },
   "outputs": [],
   "source": [
    "### A COMPLETER ###\n",
    "def mean_square_error(y_true, y_pred):\n",
    "  loss = ...\n",
    "  grad = ...\n",
    "\n",
    "  return loss, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2XnUBj2n-Df"
   },
   "source": [
    "La descente de gradient stochastique prend en entrée les paramètres suivants :  \n",
    "*    *x_train* et *y_train* respectivement les données et labels de l'ensemble d'apprentissage (que l'on suppose de taille $N$).\n",
    "*    *model* une instance du modèle que l'on veut entraîner (qui doit implanter les 3 fonctions vues précédemment *forward*, *backward* et *update_parameters*).\n",
    "*    *loss_function* peut prendre deux valeurs : 'mse' (erreur quadratique moyenne) ou 'bce' (entropie croisée binaire, que nous implémenterons par la suite).\n",
    "*    *learning_rate* le taux d'apprentissage choisi pour la descente de gradient.\n",
    "*    *epochs* le nombre de parcours complets de l'ensemble d'apprentissage que l'on veut réaliser.\n",
    "*    *batch_size* la taille de mini-batch désirée pour la descente de gradient stochastique. \n",
    "\n",
    "L'algorithme à implémenter est rappelé ci-dessous :       \n",
    "```\n",
    "N_batch = floor(N/batch_size)\n",
    "\n",
    "Répéter epochs fois\n",
    "\n",
    "  Pour b de 1 à N_batch Faire\n",
    "\n",
    "    Sélectionner les données x_train_batch et labels y_train_batch du b-ème mini-batch\n",
    "    Calculer la prédiction y_pred_batch du modèle pour ce mini-batch\n",
    "    Calculer la perte batch_loss et le gradient de la perte batch_grad par rapport aux prédictions sur ce mini-batch\n",
    "    Calculer les gradients de la perte par rapport à chaque paramètre du modèle\n",
    "    Mettre à jour les paramètres du modèle \n",
    "\n",
    "  Fin Pour\n",
    "\n",
    "Fin Répéter\n",
    "\n",
    "```\n",
    "Deux remarques additionnelles :    \n",
    "1. A chaque *epoch*, les *mini-batches* doivent être différents (les données doivent être réparties dans différents *mini-batches*).\n",
    "2. Il est intéressant de calculer (et d'afficher !) la perte moyennée sur l'ensemble d'apprentissage à chaque *epoch*. Pour cela, on peut accumuler les pertes de chaque *mini-batch* sur une *epoch* et diviser l'ensemble par le nombre de *mini-batches*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lk3lypUOLXbv"
   },
   "outputs": [],
   "source": [
    "def SGD(x_train, y_train, model, loss_function, learning_rate, epochs, batch_size):\n",
    "  # Nombre de batches par epoch\n",
    "  nb_batches = math.floor(x_train.shape[0] / batch_size)\n",
    "\n",
    "  # Pour gérer le tirage aléatoire des batches parmi les données d'entraînement, \n",
    "  # génération et permutation des indices\n",
    "  indices = np.arange(x_train.shape[0])\n",
    "  indices = np.random.permutation(indices)\n",
    "\n",
    "  for e in range(epochs):\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    for b in range(nb_batches):\n",
    "\n",
    "      # Sélection des données du batch courant\n",
    "      x_train_batch = x_train[indices[b*batch_size:(b+1)*batch_size]]\n",
    "      y_train_batch = y_train[indices[b*batch_size:(b+1)*batch_size]]\n",
    "\n",
    "      # Prédiction du modèle pour le batch courant\n",
    "      y_pred_batch = model.forward(np.transpose(x_train_batch))\n",
    "\n",
    "      # Calcul de la perte et des gradients sur le batch courant\n",
    "      if loss_function == 'mse':\n",
    "        batch_loss, batch_gradients = mean_square_error(y_train_batch, y_pred_batch)\n",
    "      elif loss_function == 'bce':\n",
    "        batch_loss, batch_gradients = binary_cross_entropy(y_train_batch, y_pred_batch)\n",
    "\n",
    "      running_loss += batch_loss \n",
    "\n",
    "      # Calcul du gradient de la perte par rapport aux paramètres du modèle\n",
    "      param_updates = model.backward(batch_gradients)\n",
    "\n",
    "      # Mise à jour des paramètres du modèle\n",
    "      model.update_parameters(param_updates, learning_rate)\n",
    "\n",
    "    print(\"Epoch %4d : Loss : %.4f\" % (e, float(running_loss/nb_batches)))\n",
    "\n",
    "    # Nouvelle permutation des données pour la prochaine epoch\n",
    "    indices = np.random.permutation(indices)\n",
    "    \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bybDhHivjXq"
   },
   "source": [
    "### Test sur un problème de régression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7q44eS0vrrZ"
   },
   "source": [
    "Le bloc de code suivant permet de générer et d'afficher des ensembles de données d'apprentissage et de test pour un problème de régression linéaire classique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "nGcIVuALraDG",
    "outputId": "35ff1cb2-ae7a-493c-ba2c-43191a64b851"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Ellipsis]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = datasets.make_regression(n_samples=250, n_features=1, n_targets=1, random_state=1, noise=10)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1/10, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=1/9, random_state=1)\n",
    "\n",
    "### A COMPLETER ###\n",
    "# Afficher un nuage de points permettant de distinguer \"Ensemble d'apprentissage\" et \"Ensemble de test\"\n",
    "# (par exemple, en traçant l'un en bleu et l'autre en rouge)\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "id": "GKFJ3c2MmomL",
    "outputId": "d5a01172-174a-4b87-9c8c-087fcb5b2f7a"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'output_size' and 'activation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pv/w96wvnnj27j18vplbhmz_yvwvgj4vl/T/ipykernel_18806/2036889914.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### A COMPLETER ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDenseLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# On pourra choisir des batchs de taille 20, pour 10 epochs et un learning-rate de 0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'output_size' and 'activation'"
     ]
    }
   ],
   "source": [
    "### A COMPLETER ###\n",
    "model = DenseLayer(...)\n",
    "model = SGD(...)\n",
    "# On pourra choisir des batchs de taille 20, pour 10 epochs et un learning-rate de 0.1\n",
    "\n",
    "# Ajouter la droite de régression linéaire obtenue sur la figure ci-dessus.\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mA9-6PqLwff4"
   },
   "source": [
    "### Test sur un problème de classification binaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9AHAgGBwjro"
   },
   "source": [
    "Afin de pouvoir tester notre perceptron mono-couche sur un problème de classification binaire (i.e. effectuer une régression logistique), il est d'abord nécessaire d'implémenter l'entropie croisée binaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_xCXP-pQb2oL"
   },
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "  loss = np.mean(- y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n",
    "  grad =  (- y_true / y_pred + (1 - y_true) / (1 - y_pred))/y_true.shape[0]\n",
    "\n",
    "  return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "4AxQRaegdntx",
    "outputId": "aacc9a92-b5d2-481d-bc04-689a56f91738"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Ellipsis]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = datasets.make_blobs(n_samples=250, n_features=2, centers=2, center_box=(- 3, 3), random_state=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1/10, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=1/9, random_state=1)\n",
    "\n",
    "### A COMPLETER ###\n",
    "# Proposer une figure permetant de distinguer \"Enssemble d'apprentissage\" et \"Ensemble de test\", \n",
    "# ainsi que les différentes classes.\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TdyntT9zSrum",
    "outputId": "769705dd-cbae-4e6f-fee9-9b341ad7c445"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'output_size' and 'activation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pv/w96wvnnj27j18vplbhmz_yvwvgj4vl/T/ipykernel_18806/1833601772.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### A COMPLETER ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDenseLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# On pourra choisir des batchs de taille 20, pour 50 epochs et un learning-rate de 0.3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'output_size' and 'activation'"
     ]
    }
   ],
   "source": [
    "### A COMPLETER ###\n",
    "model = DenseLayer(...)\n",
    "model = SGD(...)\n",
    "# On pourra choisir des batchs de taille 20, pour 50 epochs et un learning-rate de 0.3\n",
    "\n",
    "# Ajouter la droite permetant de séparer les deux classes, i.e. la droite de régression logistique,\n",
    "# obtenue sur la figure ci-dessus.\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiGyXLvum0uI"
   },
   "source": [
    "---\n",
    "\n",
    "## Perceptron multi-couche\n",
    "\n",
    "Nous allons à présent construire un perceptron multi-couches en se basant sur le perceptron mono-couche précédement développé. L'idée est d'accoler des perceptrons mono-couche en utilisant la sortie de la couche précédente comme entrée de la couche courante.\n",
    "\n",
    "En remarquant cela on peut créer une classe `MultiLayerPerceptron`. En particulier, la fonction `add_layer` permet d'ajouter au MLP le *layer* qu'elle prend en entrée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "RNhqq0KXm4Jd"
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self):\n",
    "      self.layers = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "      self.layers.append(layer)\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "\n",
    "      output_l = x_batch\n",
    "\n",
    "      for i in range(len(self.layers)):\n",
    "        # La sortie de la couche précédente est passée en entrée de la couche courante\n",
    "        input_l = output_l\n",
    "        \n",
    "        output_l = self.layers[i].forward(input_l)  \n",
    "      \n",
    "      # La sortie de la dernière couche est la sortie finale du réseau\n",
    "      y = output_l\n",
    "\n",
    "      return y\n",
    "\n",
    "    def backward(self, dy):\n",
    "      gradients = []\n",
    "      \n",
    "      for i in reversed(range(len(self.layers))):\n",
    "        # La sortie de la couche précédente est passée en entrée de la couche courante\n",
    "        layer_gradients = self.layers[i].backward(dy)  \n",
    "        gradients.append(layer_gradients) \n",
    "        dy = layer_gradients[\"dx\"]\n",
    "      \n",
    "      gradients.reverse()\n",
    "      return gradients\n",
    "\n",
    "    def update_parameters(self, gradients, learning_rate):\n",
    "      for i in range(len(self.layers)):\n",
    "        self.layers[i].update_parameters(gradients[i], learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "pijGm1ipwrAw",
    "outputId": "237f56b0-b4da-4006-8e3a-61bf5a19e45c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Ellipsis]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = datasets.make_gaussian_quantiles(n_samples=250, n_features=2, n_classes=2, random_state=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1/10, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=1/9, random_state=1)\n",
    "\n",
    "### A COMPLETER ###\n",
    "# Proposer une figure permetant de distinguer \"Enssemble d'apprentissage\" et \"Ensemble de test\", \n",
    "# ainsi que les différentes classes.\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant la fonction `add_layer` et la classe `DenseLayer`, nous pouvons à présent juxtaposer des perceptrons mono-couche, de sorte à construire un perceptron multi-couche.\n",
    "\n",
    "<img src=\"https://docs.google.com/uc?export=download&id=1F6greDVfj0-KcQKyP6rWMnO1rDH_Ck23\" height=200> \n",
    "\n",
    "Construisez le réseau ci-dessus, en choisissant une fonction d'activation ReLu pour chacune des couches cachées et sigmoïde pour la couche de sortie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3He5gXmxQ1j",
    "outputId": "fdf48193-c9e9-4282-da17-4081b1fcf183"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 : Loss : 0.6976\n",
      "Epoch    1 : Loss : 0.6942\n",
      "Epoch    2 : Loss : 0.6922\n",
      "Epoch    3 : Loss : 0.6929\n",
      "Epoch    4 : Loss : 0.6883\n",
      "Epoch    5 : Loss : 0.6871\n",
      "Epoch    6 : Loss : 0.6832\n",
      "Epoch    7 : Loss : 0.6798\n",
      "Epoch    8 : Loss : 0.6765\n",
      "Epoch    9 : Loss : 0.6681\n",
      "Epoch   10 : Loss : 0.6598\n",
      "Epoch   11 : Loss : 0.6481\n",
      "Epoch   12 : Loss : 0.6335\n",
      "Epoch   13 : Loss : 0.6119\n",
      "Epoch   14 : Loss : 0.5844\n",
      "Epoch   15 : Loss : 0.5528\n",
      "Epoch   16 : Loss : 0.5043\n",
      "Epoch   17 : Loss : 0.4426\n",
      "Epoch   18 : Loss : 0.3818\n",
      "Epoch   19 : Loss : 0.3363\n",
      "Epoch   20 : Loss : 0.2783\n",
      "Epoch   21 : Loss : 0.2371\n",
      "Epoch   22 : Loss : 0.2175\n",
      "Epoch   23 : Loss : 0.1929\n",
      "Epoch   24 : Loss : 0.1731\n",
      "Epoch   25 : Loss : 0.1552\n",
      "Epoch   26 : Loss : 0.1381\n",
      "Epoch   27 : Loss : 0.1378\n",
      "Epoch   28 : Loss : 0.1309\n",
      "Epoch   29 : Loss : 0.1215\n",
      "Epoch   30 : Loss : 0.1166\n",
      "Epoch   31 : Loss : 0.1015\n",
      "Epoch   32 : Loss : 0.1130\n",
      "Epoch   33 : Loss : 0.0991\n",
      "Epoch   34 : Loss : 0.1249\n",
      "Epoch   35 : Loss : 0.1404\n",
      "Epoch   36 : Loss : 0.1524\n",
      "Epoch   37 : Loss : 0.0883\n",
      "Epoch   38 : Loss : 0.0886\n",
      "Epoch   39 : Loss : 0.1069\n",
      "Epoch   40 : Loss : 0.0975\n",
      "Epoch   41 : Loss : 0.1109\n",
      "Epoch   42 : Loss : 0.0820\n",
      "Epoch   43 : Loss : 0.0761\n",
      "Epoch   44 : Loss : 0.0825\n",
      "Epoch   45 : Loss : 0.0701\n",
      "Epoch   46 : Loss : 0.1017\n",
      "Epoch   47 : Loss : 0.1305\n",
      "Epoch   48 : Loss : 0.1569\n",
      "Epoch   49 : Loss : 0.1296\n",
      "Epoch   50 : Loss : 0.0977\n",
      "Epoch   51 : Loss : 0.1835\n",
      "Epoch   52 : Loss : 0.1063\n",
      "Epoch   53 : Loss : 0.0789\n",
      "Epoch   54 : Loss : 0.0721\n",
      "Epoch   55 : Loss : 0.0700\n",
      "Epoch   56 : Loss : 0.0702\n",
      "Epoch   57 : Loss : 0.0765\n",
      "Epoch   58 : Loss : 0.0614\n",
      "Epoch   59 : Loss : 0.0780\n",
      "Epoch   60 : Loss : 0.1357\n",
      "Epoch   61 : Loss : 0.1034\n",
      "Epoch   62 : Loss : 0.1617\n",
      "Epoch   63 : Loss : 0.1123\n",
      "Epoch   64 : Loss : 0.2237\n",
      "Epoch   65 : Loss : 0.1059\n",
      "Epoch   66 : Loss : 0.1180\n",
      "Epoch   67 : Loss : 0.0992\n",
      "Epoch   68 : Loss : 0.0532\n",
      "Epoch   69 : Loss : 0.0840\n",
      "Epoch   70 : Loss : 0.0591\n",
      "Epoch   71 : Loss : 0.0844\n",
      "Epoch   72 : Loss : 0.0591\n",
      "Epoch   73 : Loss : 0.0713\n",
      "Epoch   74 : Loss : 0.0956\n"
     ]
    }
   ],
   "source": [
    "model = MultiLayerPerceptron()\n",
    "model.add_layer(DenseLayer(...))\n",
    "[...]\n",
    "\n",
    "model = SGD(x_train, y_train, model, 'bce', 0.1, 75, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "vjhAoTaMjqPX",
    "outputId": "91090725-a317-4f59-e486-cf1475bad62b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 61) (51, 61) (51, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pv/w96wvnnj27j18vplbhmz_yvwvgj4vl/T/ipykernel_18806/2931231555.py:19: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.\n",
      "  c = plt.pcolor(x, y, z_gen, cmap='RdBu', vmin=z_min, vmax=z_max)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make these smaller to increase the resolution\n",
    "dx, dy = 0.1, 0.1\n",
    "\n",
    "# generate 2 2d grids for the x & y bounds\n",
    "y, x = np.mgrid[slice(-2.5, 2.5 + dy, dy),\n",
    "                slice(-3, 3 + dx, dx)]\n",
    "\n",
    "\n",
    "x_gen = np.concatenate((np.expand_dims(np.reshape(y, (-1)),1),np.expand_dims(np.reshape(x, (-1)),1)), axis=1)\n",
    "z_gen = model.forward(np.transpose(x_gen)).reshape(x.shape)\n",
    "\n",
    "# x and y are bounds, so z should be the value *inside* those bounds.\n",
    "# Therefore, remove the last value from the z array.\n",
    "#z = z[:-1, :-1]\n",
    "z_min, z_max = 0, 1\n",
    "\n",
    "print(x.shape, y.shape, z_gen.shape)\n",
    "\n",
    "c = plt.pcolor(x, y, z_gen, cmap='RdBu', vmin=z_min, vmax=z_max)\n",
    "plt.colorbar(c)\n",
    "\n",
    "plt.plot(x_train[y_train==0,0], x_train[y_train==0,1], 'r.')\n",
    "plt.plot(x_train[y_train==1,0], x_train[y_train==1,1], 'b.')\n",
    "\n",
    "plt.plot(x_test[y_test==0,0], x_test[y_test==0,1], 'r+')\n",
    "plt.plot(x_test[y_test==1,0], x_test[y_test==1,1], 'b+')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le code ci-dessus, que représente *z_gen* ? Commentez le résultat obtenus."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TP Réseaux de Neurones avec Numpy.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "xxuVc",
   "launcher_item_id": "X20PE"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
